{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rjpVfgZjMAU2PgZdZtLqARa7lHO4Zi-g","timestamp":1746767665501},{"file_id":"1a030z-gCqi90dJ93LQgqy61lsbj80A5J","timestamp":1746725089738}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import librosa\n","import numpy as np\n","import pickle\n","import soundfile as sf\n","import os\n","from sklearn.linear_model import LinearRegression\n","\n","# Extract MFCC audio features\n","def extract_features(file_path):\n","    try:\n","        y, sr = librosa.load(file_path, sr=None)\n","        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","        mfcc_mean = np.mean(mfcc.T, axis=0)\n","        return mfcc_mean\n","    except Exception as e:\n","        print(f\"Feature extraction error: {e}\")\n","        return None\n","\n","# Create a dummy age model if no real one is found\n","def create_dummy_model(model_path='age_model.pkl'):\n","    print(\"Creating dummy model...\")\n","    X = np.random.rand(200, 13)\n","    y = np.random.randint(10, 61, size=200)\n","    model = LinearRegression()\n","    model.fit(X, y)\n","    with open(model_path, 'wb') as f:\n","        pickle.dump(model, f)\n","    print(\"Dummy model saved.\")\n","\n","# Predict age from audio file\n","def predict_age(audio_path, model_path='age_model.pkl'):\n","    if not os.path.exists(model_path):\n","        create_dummy_model(model_path)\n","\n","    features = extract_features(audio_path)\n","    if features is None:\n","        return \"Audio feature extraction failed.\"\n","\n","    features = features.reshape(1, -1)\n","    try:\n","        with open(model_path, 'rb') as f:\n","            model = pickle.load(f)\n","        age = model.predict(features)[0]\n","        age = np.clip(age, 10, 60)  # Keep within realistic human range\n","        return round(age, 1)\n","    except Exception as e:\n","        return f\"Prediction error: {e}\"\n","\n","# Main program\n","if __name__ == \"__main__\":\n","    audio_file = \"/common_voice_en_41910499.mp3\"  # Replace with your voice file\n","\n","    # Convert .mp3 to .wav if necessary\n","    if audio_file.endswith(\".mp3\"):\n","        try:\n","            y, sr = librosa.load(audio_file, sr=None)\n","            audio_file = \"converted.wav\"\n","            sf.write(audio_file, y, sr)\n","        except Exception as e:\n","            print(f\"MP3 to WAV conversion failed: {e}\")\n","            exit()\n","\n","    predicted_age = predict_age(audio_file)\n","    print(f\"Predicted Age: {predicted_age}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yxGquHQ7CUO","executionInfo":{"status":"ok","timestamp":1746767345325,"user_tz":-330,"elapsed":3348,"user":{"displayName":"arsath ahmed","userId":"14367044008985191739"}},"outputId":"80c51cad-1bf6-4e51-8fb6-476b4f911064"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MP3 to WAV conversion failed: [Errno 2] No such file or directory: '/common_voice_en_41910499.mp3'\n","Feature extraction error: [Errno 2] No such file or directory: '/common_voice_en_41910499.mp3'\n","Predicted Age: Audio feature extraction failed.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-353cd2130d2b>:56: UserWarning: PySoundFile failed. Trying audioread instead.\n","  y, sr = librosa.load(audio_file, sr=None)\n","/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","<ipython-input-1-353cd2130d2b>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  y, sr = librosa.load(file_path, sr=None)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mlrouig5ctww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from speechbrain.pretrained import SpeakerRecognition\n","import torchaudio\n","import torchaudio.transforms as T\n","\n","# Load pretrained speaker recognition model\n","recognizer = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n","\n","def analyze_voice(file_path):\n","    # Load audio file\n","    signal, fs = torchaudio.load(file_path)\n","\n","    # Convert to mono if stereo\n","    if signal.shape[0] > 1:\n","        signal = signal.mean(dim=0, keepdim=True)\n","\n","    # Normalize sample rate to 16kHz (if necessary)\n","    if fs != 16000:\n","        resampler = T.Resample(orig_freq=fs, new_freq=16000)\n","        signal = resampler(signal)\n","        fs = 16000\n","\n","    # Estimate gender based on pitch and characteristics\n","    avg_amplitude = signal.abs().mean().item()\n","    gender = \"Female\" if avg_amplitude > 0.03 else \"Male\"\n","    print(f\"Estimated Gender: {gender}\")\n","\n","    # Extract speaker embedding (this is not necessary for gender classification)\n","    embedding = recognizer.encode_batch(signal)\n","    print(\"Voice embedding extracted.\")\n","\n","    return gender\n","\n","# Run with your file\n","file_path = \"/common_voice_en_41910500.mp3\"  # Replace with your voice file path\n","gender = analyze_voice(file_path)\n","print(f\"The predicted gender is: {gender}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"FS27VLzd908_","executionInfo":{"status":"error","timestamp":1746767378824,"user_tz":-330,"elapsed":163,"user":{"displayName":"arsath ahmed","userId":"14367044008985191739"}},"outputId":"84a26e77-83de-4fb8-eeb9-a2519ada4dce"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'speechbrain'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ccb29a9c939f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspeechbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpeakerRecognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load pretrained speaker recognition model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'speechbrain'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"speech to text.ipynb\n","\n","Automatically generated by Colab.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1rYGttgb4k4nHVqPaHqllI9u8eVsgYWHS\n","\"\"\"\n","\n","!pip install git+https://github.com/openai/whisper.git\n","\n","import whisper\n","\n","# Load the small pretrained Whisper model\n","model = whisper.load_model(\"small\")  # Options: tiny, base, small, medium, large\n","\n","# Transcribe an audio file (must be .wav, .mp3, or .m4a)\n","result = model.transcribe(\"/content/harvard.wav\")\n","\n","# Print the transcription\n","print(\"Transcription:\\n\", result[\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBZUxlzGdwI6","outputId":"db61bff4-28c3-4f25-8a5e-66f3510dc87b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-38ywfci6\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-38ywfci6\n","  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n","Collecting tiktoken (from openai-whisper==20240930)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:01:01\u001b[0m"]}]}]}